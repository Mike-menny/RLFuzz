/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[INFO:swift] Successfully registered `/usr/local/lib/python3.10/dist-packages/swift/llm/dataset/data/dataset_info.json`.
Project structure created at: /workspace/output/projects/cjson
[INFO:swift] Setting args.remove_unused_columns: False
[INFO:swift] Successfully registered ['tools/dataset_pre.py'].
[INFO:swift] Successfully imported external_plugins: ['tools/plugin.py'].
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
[INFO:swift] Loading the model using model_dir: /workspace/models/checkpoint-600
[INFO:swift] Setting torch_dtype: torch.bfloat16
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] Setting args.dataloader_num_workers: 1
[INFO:swift] output_dir: /workspace/output/checkpoint-600/v0-20250908-212528
[INFO:swift] Global seed set to 42
[INFO:swift] args: RLHFArguments(
_n_gpu=-1,
acc_steps=1,
acc_strategy=token,
accelerator_config={'dispatch_batches': False},
adafactor=False,
adalora_beta1=0.85,
adalora_beta2=0.85,
adalora_deltaT=1,
adalora_init_r=12,
adalora_orth_reg_weight=0.5,
adalora_target_r=8,
adalora_tfinal=0,
adalora_tinit=0,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
adapter_act=gelu,
adapter_length=128,
adapters=[],
add_version=True,
agent_template=None,
aligner_lr=None,
async_generate=False,
attn_impl=None,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=True,
bf16_full_eval=False,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_storage=None,
bnb_4bit_quant_type=nf4,
bnb_4bit_use_double_quant=True,
boft_block_num=0,
boft_block_size=4,
boft_dropout=0.0,
boft_n_butterfly_factor=1,
center_rewards_coefficient=None,
channels=None,
check_model=True,
ckpt_dir=/workspace/models/checkpoint-600,
cliprange=0.2,
cliprange_value=0.2,
columns={},
completion_length_limit_scope=per_round,
cosine_max_len=None,
cosine_max_len_value_correct=0.5,
cosine_max_len_value_wrong=0.0,
cosine_min_len_value_correct=1.0,
cosine_min_len_value_wrong=-0.5,
cpo_alpha=1.0,
create_checkpoint_symlink=False,
custom_dataset_info=[],
custom_register_path=['tools/dataset_pre.py'],
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=None,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset=/workspace/datasets/prompt004.jsonl,
dataset_num_proc=1,
dataset_shuffle=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=18000000,
debug=,
deepspeed=None,
delta=None,
desirable_weight=1.0,
device_map=None,
disable_tqdm=None,
do_eval=False,
do_predict=False,
do_train=False,
download_mode=reuse_dataset_if_exists,
ds3_gather_for_generation=True,
dynamic_sample=False,
epsilon=0.2,
epsilon_high=None,
eval_accumulation_steps=None,
eval_dataset=[],
eval_dataset_args=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_generation_config=None,
eval_limit=None,
eval_on_start=False,
eval_steps=50,
eval_strategy=steps,
eval_use_evalscope=False,
eval_use_gather_object=False,
external_plugins=['tools/plugin.py'],
fourier_n_frequency=2000,
fourier_scaling=300.0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_aligner=True,
freeze_llm=False,
freeze_parameters=[],
freeze_parameters_ratio=0.0,
freeze_parameters_regex=None,
freeze_vit=True,
fsdp=,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_num=1,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
galore_cos_threshold=0.4,
galore_gamma_proj=2,
galore_optim_per_parameter=False,
galore_proj_bits=4,
galore_proj_group_size=256,
galore_proj_quant=False,
galore_proj_type=std,
galore_quantization=False,
galore_queue_size=5,
galore_rank=128,
galore_scale=1.0,
galore_target_modules=None,
galore_update_proj_gap=50,
galore_with_embedding=False,
gamma=1.0,
gc_collect_after_offload=False,
generation_batch_size=None,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hqq_axis=None,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_args_error=False,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
init_strategy=None,
init_weights=True,
interleave_prob=None,
jit_mode_eval=False,
kl_coef=0.05,
label_names=None,
label_smoothing=0,
label_smoothing_factor=0.0,
lam=0.95,
lazy_tokenize=False,
learning_rate=5e-07,
length_column_name=length,
lisa_activated_layers=0,
lisa_step_interval=20,
llamapro_num_groups=None,
llamapro_num_new_blocks=4,
lmbda=0.5,
load_args=False,
load_best_model_at_end=False,
load_data_args=False,
load_from_cache_file=True,
local_rank=-1,
local_repo_path=None,
local_rollout_forward_batch_size=64,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/checkpoint-600/v0-20250908-212528/runs,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
logprobs=False,
lora_alpha=32,
lora_bias=none,
lora_dropout=0.05,
lora_dtype=None,
lora_ga_batch_size=2,
lora_ga_direction=ArB2r,
lora_ga_iters=2,
lora_ga_max_length=1024,
lora_ga_scale=stable,
lora_ga_stable_gamma=16,
lora_modules=[],
lora_rank=8,
lorap_lr_ratio=None,
loss_scale=last_round,
loss_type=grpo,
lr_scheduler_kwargs=None,
lr_scheduler_type=cosine,
max_completion_length=4096,
max_epochs=None,
max_grad_norm=1.0,
max_length=None,
max_memory={},
max_new_tokens=4096,
max_pixels=None,
max_resample_times=3,
max_steps=-1,
max_turns=None,
metric=None,
metric_for_best_model=reward,
metric_warmup_step=0,
missing_eos_penalty=None,
model=/workspace/models/checkpoint-600,
model_author=None,
model_kwargs={},
model_name=None,
model_revision=None,
model_type=qwen2_5,
modules_to_save=[],
move_model_batches=None,
mp_parameters=,
multi_turn_func=None,
multi_turn_scheduler=None,
neftune_noise_alpha=None,
no_cuda=False,
norm_bbox=None,
num_beams=1,
num_generations=6,
num_iterations=1,
num_labels=None,
num_mini_batches=1,
num_ppo_epochs=4,
num_sample_generations=10,
num_train_epochs=50,
offload_model=False,
offload_optimizer=False,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
optimizer=None,
output_dir=/workspace/output/checkpoint-600/v0-20250908-212528,
overlong_filter=False,
overwrite_output_dir=False,
packing=False,
packing_cache=None,
padding_free=False,
padding_side=right,
past_index=-1,
per_device_eval_batch_size=6,
per_device_train_batch_size=6,
predict_with_generate=False,
prediction_loss_only=False,
problem_type=None,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_bits=None,
quant_method=None,
ray_scope=last,
ref_model=/workspace/models/checkpoint-600,
ref_model_mixup_alpha=0.6,
ref_model_revision=None,
ref_model_sync_steps=512,
ref_model_type=qwen2_5,
reft_args=None,
reft_intervention_type=LoreftIntervention,
reft_layer_key=None,
reft_layers=None,
reft_rank=4,
remove_unused_columns=False,
repetition_max_penalty=-1.0,
repetition_n_grams=3,
repetition_penalty=1.0,
report_to=['tensorboard'],
response_length=4096,
response_prefix=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
resume_only_model=False,
reward_adapters=[],
reward_funcs=['external_countdown'],
reward_model=None,
reward_model_plugin=None,
reward_model_revision=None,
reward_model_type=None,
reward_weights=None,
rlhf_type=grpo,
rope_scaling=None,
rpo_alpha=1.0,
run_name=/workspace/output/checkpoint-600/v0-20250908-212528,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
scale_rewards=True,
seed=42,
seq_kd=False,
sequence_parallel_size=1,
sft_alpha=0,
shuffle_buffer_size=1000,
simpo_gamma=1,
skip_memory_metrics=True,
sleep_level=0,
soft_cache_length=None,
soft_max_length=None,
sortish_sampler=False,
split_dataset_ratio=0.1,
steps_per_generation=None,
stop_words=[],
stopping_strategy=first_exhausted,
stream=False,
streaming=False,
strict=False,
swanlab_exp_name=None,
swanlab_lark_secret=None,
swanlab_lark_webhook_url=None,
swanlab_mode=cloud,
swanlab_project=None,
swanlab_token=<SWANLAB_TOKEN>,
swanlab_workspace=None,
sync_ref_model=False,
system=None,
target_modules=['all-linear'],
target_regex=None,
task_type=causal_lm,
teacher_adapters=[],
teacher_model=None,
teacher_model_revision=None,
teacher_model_type=None,
temperature=0.9,
template=qwen2_5,
template_backend=swift,
tf32=None,
top_k=50,
top_logprobs=None,
top_p=0.9,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_dtype=torch.bfloat16,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_dataloader_shuffle=True,
train_type=full,
trainable_parameters=[],
trainable_parameters_regex=None,
truncation_strategy=left,
tuner_backend=peft,
undesirable_weight=1.0,
use_chat_template=True,
use_cpu=False,
use_dora=False,
use_galore=False,
use_hf=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_logits_to_keep=None,
use_mps_device=False,
use_rslora=False,
use_swift_lora=False,
use_vllm=False,
val_dataset=[],
val_dataset_shuffle=False,
vera_d_initial=0.1,
vera_dropout=0.0,
vera_projection_prng_key=0,
vera_rank=256,
vf_coef=0.1,
vit_gradient_checkpointing=None,
vit_lr=None,
vllm_enable_prefix_caching=True,
vllm_enforce_eager=False,
vllm_gpu_memory_utilization=0.9,
vllm_limit_mm_per_prompt=None,
vllm_max_model_len=None,
vllm_mode=colocate,
vllm_server_base_url=None,
vllm_server_host=None,
vllm_server_port=8000,
vllm_server_timeout=240.0,
vllm_tensor_parallel_size=1,
wandb_log_unique_prompts=None,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.1,
whiten_rewards=False,
zero_hpz_partition_size=None,
)
[INFO:swift] Loading the model using model_dir: /workspace/models/checkpoint-600
[INFO:swift] Successfully loaded /workspace/models/checkpoint-600/args.json.
[INFO:swift] Loading the model using model_dir: /workspace/models/checkpoint-600
[INFO:swift] model_kwargs: {'device_map': 'auto'}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:06,  6.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.09s/it]
[INFO:swift] Loading the model using model_dir: /workspace/models/checkpoint-600
[INFO:swift] model_kwargs: {'device_map': 'auto'}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
[INFO:swift] model.hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}
[INFO:swift] model_info: ModelInfo(model_type='qwen2_5', model_dir='/workspace/models/checkpoint-600', torch_dtype=torch.bfloat16, max_model_len=32768, quant_method=None, quant_bits=None, rope_scaling=None, config=Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "pad_token_id": 151643,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": false,
  "use_sliding_window": false,
  "vocab_size": 152064
}
, task_type='causal_lm', num_labels=None)
[INFO:swift] model.generation_config: GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "max_new_tokens": 4096,
  "pad_token_id": 151643,
  "temperature": 0.9,
  "top_p": 0.9
}

[INFO:swift] default_system: 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'
[INFO:swift] max_length: 32768
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] Start time of running main: 2025-09-08 21:25:56.283595
[INFO:swift] swift.__version__: 3.6.1
[INFO:swift] train_dataset: Dataset({
    features: ['messages'],
    num_rows: 30
})
[INFO:swift] val_dataset: Dataset({
    features: ['messages'],
    num_rows: 3
})
[INFO:swift] The split dataset from the training set will be saved at: /workspace/output/checkpoint-600/v0-20250908-212528/val_dataset.jsonl.
[INFO:swift] The RLHFArguments will be saved in: /workspace/output/checkpoint-600/v0-20250908-212528/args.json
[INFO:swift] model: Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(152064, 3584, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
          (k_proj): Linear(in_features=3584, out_features=512, bias=True)
          (v_proj): Linear(in_features=3584, out_features=512, bias=True)
          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
)
[INFO:swift] model_parameter_info: Qwen2ForCausalLM: 7615.6165M Params (7615.6165M Trainable [100.0000%]), 0.0001M Buffers.
INFO 09-08 21:25:57 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 09-08 21:25:57 [__init__.py:239] Automatically detected platform cuda.
/usr/local/lib/python3.10/dist-packages/swift/trainers/mixin.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GRPOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[INFO:swift] use_reentrant: True
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
[INFO:swift] default_system: 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'
[INFO:swift] max_length: 32768
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] The logging file will be saved in: /workspace/output/checkpoint-600/v0-20250908-212528/logging.jsonl
Train:   0%|          | 0/1500 [00:00<?, ?it/s]llm has generated harnesses. now implement reward computation
Path created: /workspace/output/projects/cjson/harnesses/harness_00001
harness 1,0 Syntax check failed
harness 1,1 Syntax check failed
harness 1,2 Syntax check passed!
Project structure created at: /workspace/output/projects/cjson
Path created: /workspace/output/projects/cjson/work/fuzzer/fuzzer_00001
Fuzzer 1,2 compilation successful! Output: /workspace/output/projects/cjson/work/fuzzer/fuzzer_00001/id_00002
Project structure created at: /workspace/output/projects/cjson
Path created: /workspace/output/projects/cjson/work/fuzzer_output/fuzzer_output_00001/id_00002
Fuzzer 1,2 completed in 1.72 seconds
Output logged to: /workspace/output/projects/cjson/work/fuzzer_output/fuzzer_output_00001/id_00002/log
harness 1,3 Syntax check failed
harness 1,4 Syntax check failed
harness 1,5 Syntax check failed
Reward computation took 3.51 seconds
Train:   0%|          | 1/1500 [00:41<17:09:14, 41.20s/it]                                                          {'loss': -4e-08, 'grad_norm': 1.25, 'learning_rate': 5e-07, 'memory(GiB)': 86.06, 'train_speed(iter/s)': 0.024097, 'completions/mean_length': 658.33337402, 'completions/min_length': 555.0, 'completions/max_length': 861.0, 'completions/clipped_ratio': 0.0, 'rewards/CountdownORM/mean': -0.51282054, 'rewards/CountdownORM/std': 1.19334114, 'reward': -0.51282054, 'reward_std': 1.19334114, 'kl': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.03, 'global_step/max_steps': '1/1500', 'percentage': '0.07%', 'elapsed_time': '41s', 'remaining_time': '17h 9m 58s'}
Train:   0%|          | 1/1500 [00:41<17:09:14, 41.20s/it]Train:   0%|          | 1/1500 [00:41<17:09:14, 41.20s/it]llm has generated harnesses. now implement reward computation
Path created: /workspace/output/projects/cjson/harnesses/harness_00002
harness 2,0 Syntax check failed
harness 2,1 Syntax check failed
harness 2,2 Syntax check failed
harness 2,3 Syntax check failed
harness 2,4 Syntax check failed
harness 2,5 Syntax check failed
Reward computation took 1.98 seconds
[INFO:swift] last_model_checkpoint: None
[INFO:swift] best_model_checkpoint: None
[INFO:swift] images_dir: /workspace/output/checkpoint-600/v0-20250908-212528/images
Traceback (most recent call last):
  File "/workspace/train.py", line 69, in <module>
    grpo()
  File "/workspace/train.py", line 47, in grpo
    result = rlhf_main(
  File "/usr/local/lib/python3.10/dist-packages/swift/llm/train/rlhf.py", line 172, in rlhf_main
    return SwiftRLHF(args).main()
  File "/usr/local/lib/python3.10/dist-packages/swift/llm/base.py", line 49, in main
    result = self.run()
  File "/usr/local/lib/python3.10/dist-packages/swift/llm/train/sft.py", line 122, in run
    return self.train(trainer)
  File "/usr/local/lib/python3.10/dist-packages/swift/llm/train/sft.py", line 183, in train
    trainer.train(trainer.args.resume_from_checkpoint)
  File "/usr/local/lib/python3.10/dist-packages/swift/trainers/mixin.py", line 419, in train
    res = super().train(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/usr/local/lib/python3.10/dist-packages/swift/trainers/rlhf_trainer/grpo_trainer.py", line 1393, in training_step
    return super().training_step(model, inputs, num_items_in_batch)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 3791, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2578, in backward
    loss.backward(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 13.93 GiB. GPU 0 has a total capacity of 79.11 GiB of which 2.89 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 56.76 GiB is allocated by PyTorch, and 18.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Train:   0%|          | 1/1500 [03:21<83:44:40, 201.12s/it]
